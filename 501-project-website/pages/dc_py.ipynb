{
  "cells": [
    {
      "cell_type": "raw",
      "metadata": {},
      "source": [
        "---\n",
        "format:\n",
        "  html:\n",
        "    code-fold: true\n",
        "    toc: true\n",
        "    toc-location: right\n",
        "---"
      ],
      "id": "ce1a98aa"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Clustering from Record Data\n",
        "\n",
        "## Introduction\n",
        "The below page describes my data clustering process using record data. Data Clustering, or cluster analysis, is a type of unsupervised machine learning. It includes automatically grouping together datapoints that fit in together as groups naturally. In terms of the entire dataset, similar values are ones that are nearby each other and grouped together into clusters, and these clusters allow us to make inferences about our data. For clustering, we shall drop the target variable (Y) of recession (0 or 1), which leaves us with our feature data (X) to cluster with. We will also need to filter out our feature data, to accurately be able to perform clustering using record data. \n",
        "\n",
        "## Theory\n",
        "### K-Means Clustering\n",
        "K-Means clustering is a type of partition-based or centroid-based clustering. Each cluster of our data points contain a centroid or a cluster center. In addition, it is also known as a non-parametric clustering algorither, which means that it doesn't make strong assumptions about the form of the mapping function that maps input variables (X) onto output variables (Y). This algorithm minimizes the sum of squared distances between data points and their respective clusters centroid. \n",
        "\n",
        "To better understand the steps of K-Means clustering, we can break down the algorithm into steps:\n",
        "\n",
        "* 1. Choose k random data points and assign them as cluster centers\n",
        "* 2. For every data point, see which centroid is nearest to it using a measurement method\n",
        "* 3. Assign the data point to the closest centroid\n",
        "* 4. Repeat the previous steps for all data points until the centroid stops changing\n",
        "* 5. The algorithm has now 'converged' when there are no more changes\n",
        "\n",
        "Our model selection methods include the elbow method to choose the optimal value for 'k', or the number of clusters. This selection technique relies on two variables, intertia and distortion. Inertia is defined as the sum of squared distances of samples to their closest cluster center. Distortion, on the other hand, is calculated as the average of the squared distances from the cluster centers of their respective clusters, using Euclidean distance. After visualising the distortion and inertia values for various k values, we then select the value of k at the 'elbow' of the curve after which both inertia and distortion decrease in a linear value. An ideal model is one which has both low values of inertia and k, but both of these variables have an inverse relationship, causing a tradeoff.\n",
        "\n",
        "\n",
        "Advantages of K-means:\n",
        "\n",
        "1. Easy to understand and implement\n",
        "2. Implementable to large datasets\n",
        "3. Computationally efficient\n",
        "\n",
        "Disadvantages of K-means:\n",
        "\n",
        "1. Manually choosing k value and depending on initial values\n",
        "2. Sensitive to poor initialization of clusters\n",
        "3. Centroids get dragged due to outliers in dataset\n",
        "\n",
        "\n",
        "### DBSCAN (Density Based Spatial Clustering of Applications with Noise)\n",
        "DBSCAN is a density-based algorithm which works on the assumption that clusters are desnse regions in space separated by regions of lower density. Densely grouped data points are grouped into a single cluster. Given a set of points in some space, it groups together points that are closely packed together (points with many nearby neighbors). Outliers are marked as points that lie alone in low-density regions, whose nearest neighbors are too far away.\n",
        "\n",
        "DBSCAN can be broken down into the following steps:\n",
        "\n",
        "\n",
        "1. The algorithm arbitrarily picks a point in the dataset until all points have been visited\n",
        "2. If there are at least minimum number of points (a threshold) clustered together for a region to be considered dense, within a radius of  (a distance measure used to locate the points in the neighborhood of any point) to the point, then we consider all these points to be part of the same cluster. These minimum number of points, known as minPts, is one parameter for DBSCAN and  is another parameter.\n",
        "3. The clusters are then expanded by recursively repeating the neighborhood calculation for each neighboring point.\n",
        "\n",
        "The model selection method uses the silhouette score to select the optimal DBSCAN hyper-parameters. The silhouette value is a measure of how similar an object is to its own cluster (cohesion) compared to other clusters (separation). The silhouette ranges from −1 to +1, where a high value indicates that the object is well matched to its own cluster and poorly matched to neighboring clusters.\n",
        "\n",
        "Advantages of DBSCAN:\n",
        "\n",
        "1. It can handle noise very well\n",
        "2. It can handle clusters of different shapes and sizes\n",
        "\n",
        "Disadvantages of DBSCAN:\n",
        "\n",
        "1. DBSCAN tends to fail if the dataset contains multiple densities or varying densities\n",
        "2. It is extremely sensitive to the hyperparameters, and a slight change in hyperparameters will drastically affect DBSCAN\n",
        "3. The concept of density doesn't apply well to highly-dimensional data\n",
        "\n",
        "\n",
        "\n",
        "### Hierarchical (Agglomerative vs Divisive) Clustering\n",
        "\n",
        "Hierarchical clustering technique is different from Partitional clustering, which divides the data into non-overlapping clusters such that each data point belongs to exactly one cluster. Hierarchical clustering can be thought of a set of nested clusters organized as a hierarchical tree, visualized through dendrograms. The agglomerative model is a type of Hierarchical Clustering, and is known as bottom-up clustering. It includes starting with the points as individual clusters, and at each step, moving up the hierarchy by merging the closest pair of clusters until only one cluster is left.\n",
        "\n",
        "Hierarchical clustering’s process can be visualized with the help of a dendrograms, which are a type of tree diagram showing hierarchical relationships between different sets of data. The dendrogram can be used to decide when to stop merging the clusters or, in other words, finding the optimal number of clusters. We cut the dendrogram tree with a horizontal line at a height where the line can traverse the maximum distance up and down without intersecting the merging point.\n",
        "\n",
        "Advantages of Hierarchical Clustering:\n",
        "\n",
        "1. No need to decide how many clusters are required\n",
        "2. Easy to use and implement\n",
        "\n",
        "Disadvantages of Hierarchical Clustering:\n",
        "\n",
        "1. Can't take a step back in this algorithm\n",
        "2. High time complexity\n",
        "\n",
        "\n",
        "\n",
        "## Methods\n",
        "\n",
        "### Data Selection\n"
      ],
      "id": "62b1f3c3"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "import os"
      ],
      "id": "fa09d14b",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "os.getcwd()"
      ],
      "id": "1c5013a7",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# import the necessary packages\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "import scipy.cluster.hierarchy as sch\n",
        "import sklearn.cluster as cluster\n",
        "sns.set_theme(style=\"whitegrid\", palette='Set2')\n",
        "import warnings\n",
        "warnings.filterwarnings(\"ignore\")\n",
        "\n",
        "\n",
        "df = pd.read_csv(\"../../data/clean-data/Fredapi_clean.csv\", index_col=[0]) # read in data\n",
        "df_cluster = df.drop(['recession', 'date'], axis=1)\n",
        "df_cluster.head() # visualize first 5 rows"
      ],
      "id": "f16d324c",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "df_cluster.shape # get the number of rows and columns"
      ],
      "id": "e41b0be4",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "print(df_cluster.info()) # get column information"
      ],
      "id": "e2504673",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Feature selection and Pre-processing\n",
        "As shown in the code above, our first step is to read in the cleaned dataset. Next, we must choose our target variable, which is the binary variable of recession. Then, we clean up our dataframe into a 'cluster' dataframe, which only contain our variables of federal funds rate (interest rate charged by the federal reserve) and the unemployment rate for the US \n",
        "\n",
        "### Separate the dataset into features and labels"
      ],
      "id": "7e2678c2"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import StandardScaler \n",
        "\n",
        "X = df_cluster\n",
        "y = df['recession']\n",
        "\n",
        "# X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)"
      ],
      "id": "d05b0080",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Perform K-means Clustering\n"
      ],
      "id": "14c7c4e3"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# import relevent libraries for clustering\n",
        "from statistics import mode\n",
        "from sklearn.cluster import KMeans\n",
        "from scipy.spatial.distance import cdist\n",
        "from sklearn.metrics import accuracy_score"
      ],
      "id": "f72850f9",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# for k means clustering we will use the elbow method to find the optimal number of clusters. \n",
        "# we will use the inertia_ attribute to find the sum of squared distances of samples to their closest cluster center. \n",
        "# we will use the range of 1 to 20 clusters and plot the inertia_ values for each cluster. \n",
        "distortions = []\n",
        "inertias = []\n",
        "k = 16\n",
        "\n",
        "for k in range(1,k):\n",
        "    kmeansmodel = KMeans(n_clusters=k, init='k-means++', random_state=42)\n",
        "    kmeansmodel.fit(X)\n",
        "\n",
        "    distortions.append(sum(np.min(cdist(X, kmeansmodel.cluster_centers_, 'euclidean'), axis=1))/ X.shape[0])\n",
        "    inertias.append(kmeansmodel.inertia_)\n",
        "    evaluation=pd.DataFrame.from_records({\"Cluster\":np.arange(1,k+1), \"Distortion\":distortions, \"Inertia\":inertias})\n",
        "\n",
        "evaluation"
      ],
      "id": "9aa0591b",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# plot distortion and inertia for kmeans, you can either plot them seperately or use fig, ax = plt.subplots(1, 2) to plot them in the same figure. Suggest the optimal number of clusters based on the plot.\n",
        "evaluation.plot.line(x=\"Cluster\", subplots=True)"
      ],
      "id": "6929e418",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# Plotting clusters for best k = 3 (as per elbow method above)\n",
        "\n",
        "bestK = KMeans(n_clusters=3, init='k-means++', random_state=42)\n",
        "labels4 = bestK.fit_predict(X)\n",
        "df['kmeans_labels'] = labels4\n",
        "\n",
        "fig, ax = plt.subplots(1,2, figsize=(10,5))\n",
        "sns.scatterplot(x=\"unemployment_value\", y=\"fed_funds_value\", hue=\"recession\", data=df, ax=ax[0]).set(title='Unemployment Rate and Federal Funds Rate by Recession')\n",
        "sns.scatterplot(x=\"unemployment_value\", y=\"fed_funds_value\", hue=\"kmeans_labels\", data=df, ax=ax[1]).set(title='K-Means Clustering Plot')"
      ],
      "id": "1ccaaf21",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "According to the distortion and inertia values across the 2 graphs, we see that the initial K-means model yields k=3 clusters as the optimal number of clusters to use. The above scatterplots provide a binary representation of the two features, unemployment_value and fed_funds_value, of our feature data, to visualize the labels generated by the K-means model with k=3. Next, we'll look at how the silhouette scores of the clusters affect our conclusions for this model.\n",
        "\n",
        "### Hyper-parameter Tuning\n"
      ],
      "id": "cba8b88d"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# THIS WILL ITERATE OVER ONE HYPER-PARAMETER (GRID SEARCH) \n",
        "# AND RETURN THE CLUSTER RESULT THAT OPTIMIZES THE SILHOUETTE SCORE\n",
        "import sklearn.cluster\n",
        "\n",
        "def maximize_silhouette(X,algo=\"birch\",nmax=20,i_plot=False):\n",
        "\n",
        "    # PARAM\n",
        "    i_print=False\n",
        "\n",
        "    #FORCE CONTIGUOUS\n",
        "    X=np.ascontiguousarray(X) \n",
        "\n",
        "    # LOOP OVER HYPER-PARAM\n",
        "    params=[]; sil_scores=[]\n",
        "    sil_max=-10\n",
        "    for param in range(2,nmax+1):\n",
        "        if(algo==\"kmeans\"):\n",
        "            model = sklearn.cluster.KMeans(n_clusters=param).fit(X)\n",
        "            labels=model.predict(X)\n",
        "\n",
        "        try:\n",
        "            sil_scores.append(sklearn.metrics.silhouette_score(X,labels))\n",
        "            params.append(param)\n",
        "        except:\n",
        "            continue \n",
        "\n",
        "        if(i_print): print(param,sil_scores[-1])\n",
        "        \n",
        "        if(sil_scores[-1]>sil_max):\n",
        "             opt_param=param\n",
        "             sil_max=sil_scores[-1]\n",
        "             opt_labels=labels\n",
        "\n",
        "    print(\"OPTIMAL PARAMETER =\",opt_param)\n",
        "\n",
        "    if(i_plot):\n",
        "        fig, ax = plt.subplots()\n",
        "        ax.plot(params, sil_scores, \"-o\")  \n",
        "        ax.set(xlabel='Hyper-parameter', ylabel='Silhouette Score')\n",
        "        plt.show()\n",
        "\n",
        "    return opt_labels\n",
        "\n",
        "k_means_opt_labels=maximize_silhouette(X,algo=\"kmeans\",nmax=15, i_plot=True)"
      ],
      "id": "c923a977",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "When n_clusters = 2, the silhouette score is maximized on our entire feature data (X). This result is different from that of our elbow method, which yielded an optimal parameter of k=3 using the graphs which plot inertia and distortion along with different k values. The elbow method is used to find the 'elbow' point, where adding additional data samples does not change cluster membership much. The silhouette score allows us to determine whether there are large gaps between each sample and all other samples within the same cluster or across different clusters. The significant difference between the 2 methods is that while the elbow method only calculates euclidean distance, the silhouette method also takes into account variables such as variance, skewness, etc. \n"
      ],
      "id": "c0e75778"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "from sklearn.metrics import silhouette_samples, silhouette_score\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "import matplotlib.cm as cm\n",
        "\n",
        "X = X[[\"unemployment_value\", \"fed_funds_value\"]] \n",
        "\n",
        "range_n_clusters = [2, 3, 4, 5]\n",
        "\n",
        "for n_clusters in range_n_clusters:\n",
        "    # Create a subplot with 1 row and 2 columns\n",
        "    fig, (ax1, ax2) = plt.subplots(1, 2)\n",
        "    fig.set_size_inches(18, 7)\n",
        "\n",
        "    # The 1st subplot is the silhouette plot\n",
        "    # The silhouette coefficient can range from -1, 1 but in this example all\n",
        "    # lie within [-0.1, 1]\n",
        "    ax1.set_xlim([-0.1, 1])\n",
        "    # The (n_clusters+1)*10 is for inserting blank space between silhouette\n",
        "    # plots of individual clusters, to demarcate them clearly.\n",
        "    ax1.set_ylim([0, len(X) + (n_clusters + 1) * 10])\n",
        "\n",
        "    # Initialize the clusterer with n_clusters value and a random generator\n",
        "    # seed of 10 for reproducibility.\n",
        "    clusterer = KMeans(n_clusters=n_clusters, random_state=10)\n",
        "    cluster_labels = clusterer.fit_predict(X)\n",
        "\n",
        "    # The silhouette_score gives the average value for all the samples.\n",
        "    # This gives a perspective into the density and separation of the formed\n",
        "    # clusters\n",
        "    silhouette_avg = silhouette_score(X, cluster_labels)\n",
        "    print(\n",
        "        \"For n_clusters =\",\n",
        "        n_clusters,\n",
        "        \"The average silhouette_score is :\",\n",
        "        silhouette_avg,\n",
        "    )\n",
        "\n",
        "    # Compute the silhouette scores for each sample\n",
        "    sample_silhouette_values = silhouette_samples(X, cluster_labels)\n",
        "\n",
        "    y_lower = 10\n",
        "    for i in range(n_clusters):\n",
        "        # Aggregate the silhouette scores for samples belonging to\n",
        "        # cluster i, and sort them\n",
        "        ith_cluster_silhouette_values = sample_silhouette_values[cluster_labels == i]\n",
        "\n",
        "        ith_cluster_silhouette_values.sort()\n",
        "\n",
        "        size_cluster_i = ith_cluster_silhouette_values.shape[0]\n",
        "        y_upper = y_lower + size_cluster_i\n",
        "\n",
        "        color = cm.nipy_spectral(float(i) / n_clusters)\n",
        "        ax1.fill_betweenx(\n",
        "            np.arange(y_lower, y_upper),\n",
        "            0,\n",
        "            ith_cluster_silhouette_values,\n",
        "            facecolor=color,\n",
        "            edgecolor=color,\n",
        "            alpha=0.7,\n",
        "        )\n",
        "\n",
        "        # Label the silhouette plots with their cluster numbers at the middle\n",
        "        ax1.text(-0.05, y_lower + 0.5 * size_cluster_i, str(i))\n",
        "\n",
        "        # Compute the new y_lower for next plot\n",
        "        y_lower = y_upper + 10  # 10 for the 0 samples\n",
        "\n",
        "    ax1.set_title(\"The silhouette plot for the various clusters.\")\n",
        "    ax1.set_xlabel(\"The silhouette coefficient values\")\n",
        "    ax1.set_ylabel(\"Cluster label\")\n",
        "\n",
        "    # The vertical line for average silhouette score of all the values\n",
        "    ax1.axvline(x=silhouette_avg, color=\"red\", linestyle=\"--\")\n",
        "\n",
        "    ax1.set_yticks([])  # Clear the yaxis labels / ticks\n",
        "    ax1.set_xticks([-0.1, 0, 0.2, 0.4, 0.6, 0.8, 1])\n",
        "\n",
        "    # 2nd Plot showing the actual clusters formed\n",
        "    colors = cm.nipy_spectral(cluster_labels.astype(float) / n_clusters)\n",
        "    ax2.scatter(\n",
        "        X['unemployment_value'], X['fed_funds_value'], marker=\".\", s=30, lw=0, alpha=0.7, c=colors, edgecolor=\"k\"\n",
        "    )\n",
        "\n",
        "    # Labeling the clusters\n",
        "    centers = clusterer.cluster_centers_\n",
        "    # Draw white circles at cluster centers\n",
        "    ax2.scatter(\n",
        "        centers[:, 0],\n",
        "        centers[:, 1],\n",
        "        marker=\"o\",\n",
        "        c=\"white\",\n",
        "        alpha=1,\n",
        "        s=200,\n",
        "        edgecolor=\"k\",\n",
        "    )\n",
        "\n",
        "    for i, c in enumerate(centers):\n",
        "        ax2.scatter(c[0], c[1], marker=\"$%d$\" % i, alpha=1, s=50, edgecolor=\"k\")\n",
        "\n",
        "    ax2.set_title(\"Visualization of the Clustered Continuous Features\")\n",
        "    ax2.set_xlabel(\"unemployment_value\")\n",
        "    ax2.set_ylabel(\"fed_funds_value\")\n",
        "\n",
        "    plt.suptitle(\n",
        "        \"Silhouette analysis for KMeans clustering on Continuous Features with n_clusters = %d\"\n",
        "        % n_clusters,\n",
        "        fontsize=14,\n",
        "        fontweight=\"bold\",\n",
        "    )\n",
        "\n",
        "plt.show()"
      ],
      "id": "22fc1575",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Silhouette plots seem to have an edge over the elbow method as using silhouette plots, one can evaluate clusters based on multiple criteria, including fluctuations in the size of the plot, non-uniform thickness, and the average Silhouette score (red dotted line). Thus, one can determine the optimal value for k using the above plots.\n",
        "\n",
        "All n_clusters, from 2 to 5, have silhouette scores above that of the average silhouete score, which is the red dotted line. This means that we would need to look at the thickness fluctuations in the plots. When n_clusters = 2, the thickness of the silhouette plots suggests that the clusters are non-uniform.\n",
        "\n",
        "For n_clusters = 3, the silhouette score is maximized, and the thickness of the silhouette plots is somewhat uniform. Hence, they clusters would be of similar sizes.\n",
        "\n",
        "### Final results for K-mean Clustering\n",
        "\n",
        "When we cluster our entire feature data (X) we choose the optimal value of k to be 3, and this is based on the elbow method, the silhouette scores, and the silhouette plots. The code below is our final result for K-means clustering, visualizing 3 different clusters.\n"
      ],
      "id": "f1f23f40"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# Plotting clusters for best k = 3 (as per silhouette method)\n",
        "\n",
        "bestK = KMeans(n_clusters=3, init='k-means++', random_state=42)\n",
        "kmeans_labels_final = bestK.fit_predict(X)\n",
        "df['kmeans_final_labels'] = kmeans_labels_final\n",
        "\n",
        "fig, ax = plt.subplots(1,2,figsize=(10,5))\n",
        "sns.scatterplot(x=\"unemployment_value\", y=\"fed_funds_value\", hue=\"recession\", data=df, ax=ax[0]).set(title='Unemployment Rate and Federal Funds Rate by Recession')\n",
        "sns.scatterplot(x=\"unemployment_value\", y=\"fed_funds_value\", hue=\"kmeans_final_labels\", data=df, ax=ax[1]).set(title='Final K-Means Clustering Plot')"
      ],
      "id": "1957e838",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Perform DBSCAN Clustering\n"
      ],
      "id": "9752d4eb"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "from sklearn.cluster import DBSCAN\n",
        "\n",
        "X = df_cluster\n",
        "model = DBSCAN(eps=0.5, min_samples=5) \n",
        "model.fit(X)\n",
        "y_pred = model.fit_predict(X)\n",
        "labels_DB = model.labels_"
      ],
      "id": "fae0333c",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "from sklearn.metrics import silhouette_score\n",
        "from sklearn import metrics\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "\n",
        "scores = []\n",
        "nums = range(1, 10)\n",
        "num_clusters = []\n",
        "samples = range(1, 100)\n",
        "num_samples = []\n",
        "for i in nums:\n",
        "    for j in samples:\n",
        "        model = DBSCAN(eps=i, min_samples=j).fit(X)\n",
        "        labels_DB = model.labels_ \n",
        "        try:\n",
        "            score = silhouette_score(X, labels_DB)\n",
        "            num_clusters.append(i)\n",
        "            scores.append(score)\n",
        "            num_samples.append(j)\n",
        "\n",
        "        except ValueError:\n",
        "            continue\n",
        "        \n",
        "\n",
        "sil_df = pd.DataFrame({\"number_of_clusters\":num_clusters, \"minimum_samples\":num_samples, \"silhouette_score\":scores})\n",
        "# sil_df.plot.scatter(x=\"minimum_samples\", y=\"silhouette_score\")\n",
        "sns.scatterplot(data=sil_df, x=\"minimum_samples\", y=\"silhouette_score\", hue=\"number_of_clusters\")"
      ],
      "id": "9ff02d52",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "centers = [[1, 1], [-1, -1], [1, -1]]\n",
        "X = df_cluster\n",
        "labels_DB = model.labels_\n",
        "labels_true = y\n",
        "\n",
        "X = StandardScaler().fit_transform(X)"
      ],
      "id": "30d68e06",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "db = DBSCAN(eps=0.3, min_samples=10).fit(X)\n",
        "core_samples_mask = np.zeros_like(db.labels_, dtype=bool)\n",
        "core_samples_mask[db.core_sample_indices_] = True\n",
        "labels = db.labels_\n",
        "\n",
        "# Number of clusters in labels, ignoring noise if present.\n",
        "n_clusters_ = len(set(labels)) - (1 if -1 in labels else 0)\n",
        "n_noise_ = list(labels).count(-1)\n",
        "\n",
        "print(\"Estimated number of clusters: %d\" % n_clusters_)\n",
        "print(\"Estimated number of noise points: %d\" % n_noise_)\n",
        "print(\"Homogeneity: %0.3f\" % metrics.homogeneity_score(labels_true, labels))\n",
        "print(\"Completeness: %0.3f\" % metrics.completeness_score(labels_true, labels))\n",
        "print(\"V-measure: %0.3f\" % metrics.v_measure_score(labels_true, labels))\n",
        "print(\"Adjusted Rand Index: %0.3f\" % metrics.adjusted_rand_score(labels_true, labels))\n",
        "print(\n",
        "    \"Adjusted Mutual Information: %0.3f\"\n",
        "    % metrics.adjusted_mutual_info_score(labels_true, labels)\n",
        ")\n",
        "print(\"Silhouette Coefficient: %0.3f\" % metrics.silhouette_score(X, labels))"
      ],
      "id": "1c4c69b3",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "The above information corresponds to the performance of the DBSCAN model we chose initially, with default hyper-parameter values. The silhouette score is -0.109, which signifies that the data points are not well clustered to their own cluster, in comparison to other clusters. However, the model generates 3 clusters, which is the same result yielded by K-means clustering which was performed above.\n",
        "\n",
        "Now, we will try to visualize the silhouette scores along with the number of clusters.\n"
      ],
      "id": "0fd14ef0"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "sns.lineplot(data=sil_df, x=\"minimum_samples\", y=\"silhouette_score\")"
      ],
      "id": "f55399c1",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "sns.lineplot(data=sil_df, x=\"number_of_clusters\", y=\"silhouette_score\")"
      ],
      "id": "64a350e2",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Hyper-parameter tuning\n"
      ],
      "id": "1bdc51d7"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# Defining the list of hyperparameters to try\n",
        "import sklearn.metrics as metrics\n",
        "# Defining the list of hyperparameters to try\n",
        "eps_list=np.arange(start=0.01, stop=4, step=0.5)\n",
        "min_sample_list=np.arange(start=5, stop=10, step=1)\n",
        " \n",
        "# Creating empty data frame to store the silhouette scores for each trials\n",
        "silhouette_scores_data=pd.DataFrame()\n",
        " \n",
        "for eps_trial in eps_list:\n",
        "    for min_sample_trial in min_sample_list:\n",
        "        \n",
        "        # Generating DBSAN clusters\n",
        "        db = DBSCAN(eps=eps_trial, min_samples=min_sample_trial)\n",
        "        \n",
        "        if(len(np.unique(db.fit_predict(X)))>=2):\n",
        "            sil_score=silhouette_score(X, db.fit_predict(X))\n",
        "        else:\n",
        "            continue\n",
        "        trial_parameters=\"eps:\" + str(eps_trial.round(1)) +\", min_sample:\" + str(min_sample_trial)\n",
        "        \n",
        "        silhouette_scores_data=silhouette_scores_data.append(pd.DataFrame(data=[[sil_score,trial_parameters]], columns=[\"score\", \"parameters\"]))\n",
        " \n",
        "# Finding out the best hyperparameters with highest Score\n",
        "silhouette_scores_data.sort_values(by='score', ascending=False).head(10)"
      ],
      "id": "818695f6",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "fig, ax = plt.subplots()\n",
        "sns.lineplot(x=\"parameters\", y=\"score\", data=silhouette_scores_data[silhouette_scores_data[\"score\"] > 0.45].reset_index())  \n",
        "ax.set(xlabel='Hyper-parameter (Epsilon & Min_Samples)', ylabel='Silhouette Score')\n",
        "plt.xticks(rotation='vertical')\n",
        "# Pad margins so that markers don't get clipped by the axes\n",
        "plt.margins(0.2)\n",
        "# Tweak spacing to prevent clipping of tick-labels\n",
        "plt.subplots_adjust(bottom=0.15)\n",
        "plt.show()"
      ],
      "id": "58a6bd91",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "A larger epsilon value produces broader clusters (encompassing more data points), and a smaller epsilon will build smaller clusters. Our silhouette plot does not show much variation in the scores based on different eps values and min_samples values. Therefore, we will choose the hyper-parameters eps = 1, and min_samples = 5, as we prefer smaller values such that we have only a small fraction of data points within the epsilon distance from each other.\n",
        "\n",
        "\n",
        "### Final Results for DBSCAN\n"
      ],
      "id": "b77f5192"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "model = DBSCAN(eps=1, min_samples=5) # best hyper-parameter values\n",
        "model.fit(X)\n",
        "y_pred = model.fit_predict(X)\n",
        "labels_DB = model.labels_\n",
        "\n",
        "df['DBSCAN_final_labels'] = labels_DB\n",
        "\n",
        "fig, ax = plt.subplots(1,2, figsize=(10,5))\n",
        "sns.scatterplot(x=\"unemployment_value\", y=\"fed_funds_value\", hue=\"recession\", data=df, ax=ax[0]).set(title='Unemployment and Fed Funds Rates by Recession (Y)')\n",
        "sns.scatterplot(x=\"unemployment_value\", y=\"fed_funds_value\", hue=\"DBSCAN_final_labels\", data=df[df['DBSCAN_final_labels'] != -1 ], ax=ax[1]).set(title='Final DBSCAN Clustering Plot') # removing label = -1 because it corresponds to noise"
      ],
      "id": "899e5431",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# Number of clusters in labels, ignoring noise if present.\n",
        "import sklearn.metrics as metrics\n",
        "from sklearn.metrics import homogeneity_score, completeness_score, v_measure_score, adjusted_rand_score\n",
        "n_clusters_ = len(set(labels_DB)) - (1 if -1 in labels_DB else 0)\n",
        "n_noise_ = list(labels_DB).count(-1)\n",
        "labels_true = y\n",
        "\n",
        "print(\"Estimated number of clusters: %d\" % n_clusters_)\n",
        "print(\"Estimated number of noise points: %d\" % n_noise_)\n",
        "print(\"Homogeneity: %0.3f\" % metrics.homogeneity_score(labels_true, labels_DB))\n",
        "print(\"Completeness: %0.3f\" % metrics.completeness_score(labels_true, labels_DB))\n",
        "print(\"V-measure: %0.3f\" % metrics.v_measure_score(labels_true, labels_DB))\n",
        "print(\"Adjusted Rand Index: %0.3f\" % metrics.adjusted_rand_score(labels_true, labels_DB))\n",
        "print(\n",
        "    \"Adjusted Mutual Information: %0.3f\"\n",
        "    % metrics.adjusted_mutual_info_score(labels_true, labels_DB)\n",
        ")\n",
        "print(\"Silhouette Coefficient: %0.3f\" % metrics.silhouette_score(X, labels_DB))"
      ],
      "id": "52241a43",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Agglomerative Clustering (Hierarchical clustering)\n"
      ],
      "id": "dba505ce"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# Perform Agglomerative Clustering\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import StandardScaler \n",
        "from scipy.cluster.hierarchy import dendrogram, linkage\n",
        "from sklearn.cluster import AgglomerativeClustering\n",
        "import sklearn.metrics\n",
        "import sklearn.cluster\n",
        "\n",
        "X = df_cluster\n",
        "y = df['recession']\n",
        "\n",
        "model = AgglomerativeClustering(n_clusters=3, affinity='euclidean',linkage='ward').fit(X)\n",
        "labels = model.labels_"
      ],
      "id": "9b81cb31",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Plot Clusters\n"
      ],
      "id": "b94eb365"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "Z = linkage(X, method='ward') # linkage computed using euclidean distance  \n",
        "dend = dendrogram(Z)\n",
        "plt.axhline(y=30000, color='r', linestyle='--', label='30000')"
      ],
      "id": "b90dd655",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "From the above dendogram, we set a threshold at value 30,000, and this cuts through the dendogram three times, giving us three clusters.\n",
        "\n",
        "### Hyper-parameter Tuning\n"
      ],
      "id": "fabcf418"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "def maximize_silhouette(X,algo=\"birch\",nmax=20,i_plot=False):\n",
        "\n",
        "    # PARAM\n",
        "    i_print=False\n",
        "\n",
        "    #FORCE CONTIGUOUS\n",
        "    X=np.ascontiguousarray(X) \n",
        "\n",
        "    # LOOP OVER HYPER-PARAM\n",
        "    params=[]; sil_scores=[]\n",
        "    sil_max=-10\n",
        "    for param in range(2,nmax+1):\n",
        "\n",
        "        if(algo==\"ag\"):\n",
        "            model = sklearn.cluster.AgglomerativeClustering(n_clusters=param, affinity=\"cosine\", linkage='single').fit(X)\n",
        "            labels=model.labels_\n",
        "\n",
        "        try:\n",
        "            sil_scores.append(sklearn.metrics.silhouette_score(X,labels))\n",
        "            params.append(param)\n",
        "        except:\n",
        "            continue \n",
        "\n",
        "        if(i_print): print(param,sil_scores[-1])\n",
        "        \n",
        "        if(sil_scores[-1]>sil_max):\n",
        "             opt_param=param\n",
        "             sil_max=sil_scores[-1]\n",
        "             opt_labels=labels\n",
        "\n",
        "    print(\"OPTIMAL PARAMETER =\",opt_param)\n",
        "\n",
        "    if(i_plot):\n",
        "        fig, ax = plt.subplots()\n",
        "        ax.plot(params, sil_scores, \"-o\")  \n",
        "        ax.set(xlabel='N_Clusters', ylabel='Silhouette Score')\n",
        "        plt.show()\n",
        "\n",
        "    return opt_labels"
      ],
      "id": "4897f0dd",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "def plot(X,color_vector):\n",
        "    fig, [ax1, ax2] = plt.subplots(1,2, figsize=(10,5))\n",
        "    sns.scatterplot(x=\"unemployment_value\", y=\"fed_funds_value\", hue=\"recession\", data=df, ax=ax1).set(title='Unemployment Rate and Federal Funds Rate by Recession')\n",
        "    ax1.set(xlabel='unemployment_value', ylabel='fed_funds_value',\n",
        "    title='Unemployment Rate and Fed Funds Rate by Recession (Y)')\n",
        "    ax1.grid()\n",
        "    scatter2 = ax2.scatter(X['unemployment_value'], X['fed_funds_value'],c=color_vector, alpha=0.5) \n",
        "    ax2.set(xlabel='unemployment_value', ylabel='fed_funds_value',\n",
        "    title='Agglomerative Clustering Plot')\n",
        "    legend2 = ax2.legend(*scatter2.legend_elements(),\n",
        "                    loc=\"lower right\", title=\"Clusters\")\n",
        "    ax2.add_artist(legend2)\n",
        "    ax2.grid()\n",
        "    plt.show()"
      ],
      "id": "8fb81c67",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "opt_labels=maximize_silhouette(X,algo=\"ag\",nmax=7, i_plot=True)\n",
        "plot(X,opt_labels)"
      ],
      "id": "ab4d5bfd",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Single-linkage uses the minimum of the distances between all observations of the two sets. For the above model, we also chose the affinity hyper-parameter, a metric used to compute the linkages, as cosine and found the the best number of clusters is five, as shown above in the first plot.\n",
        "\n",
        "## Results\n",
        "\n",
        "We performed clustering using three different methods, namely K-means, DBSCAN, and Agglomerative Clustering. The final result of the K-means model was based on the outputs of both the silhouette method and the elbow method, in adddition to an analysis of the silhouette plots for various values of k. The elbow method conveyes that k = 3 was the best parameter, but the silhouette method then yielded a value for k = 2 to be optimal. However, when we dove deeper into this, the silhouette score plots showed us that the ideal k was k = 3, as the silhouette plots were relatively even, in addition to not having a significantly different silhouette score compared to when k = 2. \n",
        "\n",
        "The DBSCAN model performed the worst of the 3 clustering models. The initial parameter values yielded a silhouette score of -0.109, which signified that the data points are not well clustered to their own cluster, in comparison to other clusters. While the model yielded the same number of clusters as the K-means clustering model, which was 3. Furthermore, after hyperparameter tuning and testing for various values for eps and min_samples, we were not able to find meaningful differences in silhouette scores, thus proving that the model was not strong to cluster our data points.\n",
        "\n",
        "Lastly, Agglomerative/Hierarchical clustering did a better job of clustering the feature data (X) than DBSCAN, but was still not as effective as K-means. By hyper-parameter turning in the Hierarchical clustering, we found our optimal paramter to be 5, yielding a silhouette score of ~0.15.\n",
        "\n",
        "## Conclusion\n",
        "\n",
        "To wrap up the clutering methods, we found that the K-means model clustered the feature data (X) the best of the 3 methods. The DBSCAN clustering performed the worst, and was not able to cluster data well, and the Hierarchical clustering model performed the second best. The main takeaway from this set of models was that although these clustering models are simple and easy to execute, they can be highly powerful to generate accurate insights from data, particularly if the right model is accurately chosen. The number of data points is crucial to these models, as a high number of data points will make the model better cluster the overall data. Additionally, more numeric features would also helpef the model, and would have to led to not only more clusters being formed, but clusters of various sizes and densities which would yield even more insightful results and takeaways.\n",
        "\n",
        "\n",
        "## References\n",
        "\n",
        "Bock, Tim. “What Is a Dendrogram?” Displayr, September 13, 2022. https://www.displayr.com/what-is-dendrogram/.\n",
        "\n",
        "Hashmi, Farukh. “How to Create Clusters Using DBSCAN in Python.” Thinking Neuron, November 27, 2021. https://thinkingneuron.com/how-to-create-clusters-using-dbscan-in-python/.\n",
        "\n",
        "“Demo of DBSCAN Clustering Algorithm.” scikit. Accessed November 12, 2022. https://scikit-learn.org/stable/auto_examples/cluster/plot_dbscan.html."
      ],
      "id": "2111d122"
    }
  ],
  "metadata": {
    "kernelspec": {
      "name": "python3",
      "language": "python",
      "display_name": "Python 3 (ipykernel)"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}