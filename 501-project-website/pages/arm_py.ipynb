{
  "cells": [
    {
      "cell_type": "raw",
      "metadata": {},
      "source": [
        "---\n",
        "format:\n",
        "  html:\n",
        "    code-fold: true\n",
        "    toc: true\n",
        "    toc-location: right\n",
        "---"
      ],
      "id": "3dcab1e5"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Associative Rule Mining (ARM)\n",
        "\n",
        "## Introduction\n",
        "\n",
        "The page below will describe my associative rule mining analysis, which was conducted using data retrieved from the Twitter API.\n",
        "\n",
        "ARM, or Associative Rule Mining, is a critical tool in Data Science which aids in evaluating several business decisions. Data mining is the process of deriving trends, patterns, and useful information from a massive amount of data. The data mining process of discovering the rules that govern associations and causal objects between sets of items is known as Associative Rule Mining. It is useful in discovering relationships between databases that seem to be independent, thereby unveiling relationships in data that we didn't know previously existed.\n",
        "\n",
        "Associative Rule Mining is a technique which is used to identify frequent patterns, correlations, and associations in datasets found in a variety of databases, including relational databases, transcational databases, and other types of repositories. Associative Rule Mining, contrary to most data models, is appropriate for non-numeric, categorical data, and requires more than simple counting. Thus, given a set of transactions, the goal of ARM is to find the rules that predict the occurrence of a specific item, based on the occurrence of other items in the same dataset. \n",
        "\n",
        "## Theory\n",
        "\n",
        "### Apriori Algorithm for ARM\n",
        "\n",
        "Several statistical algorithms have been developed to implement associative rule mining, and Apriori is one such algorithm which is used for ARM. In the following section, we will learn about the theory behind the Apriori algorithm, and later see its implementation in python using data extracted from a News API.\n",
        "\n",
        "There are three main components of the Apriori Algorithm:\n",
        "\n",
        "1. Support\n",
        "\n",
        "2. Confidence\n",
        "\n",
        "3. Lift\n",
        "\n",
        "#### Support\n",
        "\n",
        "Support refers to the default popularity of an item and can be calculated by finding the number of transactions containing a particular item, divided by the total number of transactions. Suppose, we wanted to evaluate the support for item X, we would calculate it as:\n",
        "\n",
        "Support(X) = (Transactions containing X)/(Total Transactions)\n",
        "\n",
        "#### Confidence\n",
        "\n",
        "Confidence refers to the likelihood that an item Y exists if an item X exists as well. It can be computed by finding the number of transactions where X and Y exist together, divided by the total number of transactions whihc contains X. Mathematically, it would be calculated as:\n",
        "\n",
        "Confidence(Y,X) = (Transactions containing both X and Y)/(Transactions containing X)\n",
        "\n",
        "#### Lift\n",
        "\n",
        "Lift(X,Y) refers to the increase in the ratio of prevalence of Y when A exists. Lift(X,Y) can be calculated by dividing Confidence(X,Y) divided by Support(Y). Mathematically, it can be represented as:\n",
        "\n",
        "Lift(X,Y) = (Confidence(X,Y))/(Support(Y))\n",
        "\n",
        "The concept of lift basically tells us the ratio of the likelihood of X and Y together in relation to the likelihood of just Y. A lift of 1 means that there is no association between X and Y. A lift which is greater than 1 tells as that X and Y are likely to exist together. Finally, a lift which is less than 1 tells us that X and Y are not likely to exist together.\n",
        "\n",
        "### Steps Involved in Apriori Algorithm\n",
        "\n",
        "For large sets of data, which are used in most cases in the field of Data Science, there can be hundreds of items and thus, several thousands of relationships between items using each possible combination of items. This process can be extrmeely slow due to the several thousands of combinations that exist, and thus, some steps need to be performed to speed up this process of conducting the Apriori Algorithm:\n",
        "\n",
        "1. Compute the support for each individual item\n",
        "\n",
        "2. Decide the support threshold\n",
        "\n",
        "3. Select the frequent items\n",
        "\n",
        "4. Find the support of the frequent items\n",
        "\n",
        "5. Repeat for larger datasets\n",
        "\n",
        "6. Generate association rules and compute confidence\n",
        "\n",
        "7. Compute lift\n",
        "\n",
        "## Code\n"
      ],
      "id": "159f603c"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# Imports\n",
        "import nltk\n",
        "import string\n",
        "from nltk.stem import WordNetLemmatizer\n",
        "from nltk.stem import PorterStemmer\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.tokenize import word_tokenize, sent_tokenize\n",
        "from nltk.sentiment import SentimentIntensityAnalyzer\n",
        "import os\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from apyori import apriori\n",
        "import networkx as nx \n",
        "import warnings\n",
        "nltk.download('stopwords')\n",
        "nltk.download('vader_lexicon')\n",
        "\n",
        "warnings.filterwarnings(\"ignore\")"
      ],
      "id": "dd547796",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "import os\n",
        "os.chdir('/Users/raunakadvani/anly-501-project-raunakadvani2410/501-project-website/pages')"
      ],
      "id": "17f3960f",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "news_df = pd.read_csv('../../data/clean-data/news_clean.csv')\n",
        "df = news_df.drop(['Unnamed: 0', 'sentiment_rating', 'label'], axis = 1)\n",
        "df = df.rename(columns={'Content_Lemmatized_Sentiment_Analysis': 'final_news'})"
      ],
      "id": "8c12938c",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "df.head(5) # visualise first 5 rows"
      ],
      "id": "3a76a17b",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# USER PARAM\n",
        "input_path          =   df\n",
        "compute_sentiment   =   True        \n",
        "sentiment           =   []          # average sentiment of each chunk of text \n",
        "ave_window_size     =   250         # size of scanning window for moving average\n",
        "                    \n",
        "\n",
        "# OUTPUT FILE\n",
        "output='transactions.txt'\n",
        "if os.path.exists(output): os.remove(output)\n",
        "\n",
        "# INITIALIZE\n",
        "lemmatizer  =   WordNetLemmatizer()\n",
        "ps          =   PorterStemmer()\n",
        "sia         =   SentimentIntensityAnalyzer()\n",
        "\n",
        "# ADD MORE\n",
        "stopwords   =   stopwords.words('english')\n",
        "add=['mr','mrs','wa','dr','said','back','could','one','looked','like','know','around','dont']\n",
        "for sp in add: stopwords.append(sp)\n",
        "\n",
        "def read_and_clean(path,START=0,STOP=-1):\n",
        "    \n",
        "    global sentiment\n",
        "\n",
        "    sentences =  []\n",
        "    for sentence in path['df']:\n",
        "        sentences.append(sentence) \n",
        "\n",
        "    print(\"NUMBER OF SENTENCES FOUND:\",len(sentences));\n",
        "\n",
        "    # CLEAN AND LEMMATIZE\n",
        "    keep='0123456789abcdefghijklmnopqrstuvwxy'\n",
        "\n",
        "    new_sentences=[];vocabulary=[]\n",
        "    \n",
        "    for sentence in sentences:\n",
        "        new_sentence=''\n",
        "\n",
        "        # REBUILD LEMMATIZED SENTENCE\n",
        "        for word in sentence.split():\n",
        "            \n",
        "            # ONLY KEEP CHAR IN \"keep\"\n",
        "            tmp2=''\n",
        "            for char in word: \n",
        "                if(char in keep): \n",
        "                    tmp2=tmp2+char\n",
        "                else:\n",
        "                    tmp2=tmp2+' '\n",
        "            word=tmp2\n",
        "\n",
        "            #-----------------------\n",
        "            # LEMMATIZE THE WORDS\n",
        "            #-----------------------\n",
        "\n",
        "            new_word = lemmatizer.lemmatize(word)\n",
        "\n",
        "            # REMOVE WHITE SPACES\n",
        "            new_word=new_word.replace(' ', '')\n",
        "\n",
        "            # BUILD NEW SENTENCE BACK UP\n",
        "            if new_word not in stopwords:\n",
        "                if new_sentence=='':\n",
        "                    new_sentence=new_word\n",
        "                else:\n",
        "                    new_sentence=new_sentence+','+new_word\n",
        "                if new_word not in vocabulary: vocabulary.append(new_word)\n",
        "\n",
        "        # SAVE (LIST OF LISTS)      \n",
        "        new_sentences.append(new_sentence.split(\",\"))\n",
        "        \n",
        "        # SIA\n",
        "        if(compute_sentiment):\n",
        "            #-----------------------\n",
        "            # USE NLTK TO DO SENTIMENT ANALYSIS \n",
        "            #-----------------------\n",
        "\n",
        "            text1=new_sentence.replace(',', ' ')\n",
        "            ss = sia.polarity_scores(text1)\n",
        "            sentiment.append([ss['neg'], ss['neu'], ss['pos'], ss['compound']])\n",
        "            \n",
        "        # SAVE SENTENCE TO OUTPUT FILE\n",
        "        if(len(new_sentence.split(','))>2):\n",
        "            f = open(output, \"a\")\n",
        "            f.write(new_sentence+\"\\n\")\n",
        "            f.close()\n",
        "\n",
        "    sentiment=np.array(sentiment)\n",
        "    print(\"TOTAL AVERAGE SENTIMENT: \",np.mean(sentiment,axis=0))\n",
        "    print(\"VOCAB LENGTH: \",len(vocabulary))\n",
        "    return new_sentences"
      ],
      "id": "1bae97dd",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "transactions = read_and_clean(input_path, 400, -400)\n",
        "texT = pd.DataFrame(transactions)\n",
        "texT.head()"
      ],
      "id": "7859f6a8",
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "name": "python3",
      "language": "python",
      "display_name": "Python 3 (ipykernel)"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}